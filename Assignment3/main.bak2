\documentclass{report}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{textcomp}
\usepackage[english]{babel}
\usepackage{amsmath, amssymb}
\usepackage{enumitem}
\usepackage{bm}
% figure support
\usepackage{import}
\usepackage{xifthen}
\pdfminorversion=7
\usepackage{pdfpages}
\usepackage{transparent}

% \newcommand{\incfig}[1]{%
% 	\def\svgwidth{\columnwidth}
% 	\import{./figures/}{#1.pdf_tex}
% }
\pdfsuppresswarningpagegroup=1

\input{preamble}
\input{macros}
\input{letterfonts}
\title{\Huge{MATH 231 : Numerical ODEs}}
\author{\huge{Pratham Lalwani}}
\date{\today}



\begin{document}
\maketitle
\newpage

\pdfbookmark[section]{\contentsname}{toc}
% \tableofcontents
\pagebreak
\qs{Polynomial Interpolation}{

	\begin{enumerate}[label=(\alph*)]
		\item Consider the data $\{(-1,-5),(0,1),(1,3)\}$. This exercise is to compute by hand the Lagrange interpolant $f_{n}(x)$.\\
		      \begin{enumerate}[label=(\roman*)]
			      \item Compute $f_{2}(x)$ by hand using Newton basis functions and verify it interpolates the data.\\
			      \item Suppose a new data point $(2,7)$ is added, repeat (i) and verify $f_{3}(x)$ interpolates the data.\\
		      \end{enumerate}
		\item Instead, suppose you have derivative information on the data $\{(-1,-5,4),(0,1,1),(2,7,121)\}$, compute by hand the Hermite interpolant $H_{5}(x)$ and verify that it interpolates the data.\\
		\item Recall for $n+1$ data points $\left\{\left(x_{i}, y_{i}\right)\right\}_{i=0}^{n}$, the Lagrange interpolant written in terms of Lagrange basis is given by
		      \begin{equation}
			      f_{n}(x)=\sum_{j=0}^{n} y_{j} \ell_{j}(x), \quad \text { where } \ell_{j}(x)=\frac{\prod_{j \neq i=0}^{n}\left(x-x_{i}\right)}{\prod_{j \neq i=0}^{n}\left(x_{j}-x_{i}\right)} . \label{one}
		      \end{equation}


		      This exercise is about evaluating $f_{n}(x)$ more efficiently on many different values of $x$, such as when plotting $f_{n}(x)$.\\
		      \begin{enumerate}[label=(\roman*)]
			      \item Show that evaluating $f_{n}(x)$ at a fixed value of $x$ using (1) directly involves $\mathcal{O}\left(n^{2}\right)$ costs.\\
			      \item Instead, deduce that \ref{one} can be rewritten as


			            \begin{equation*}
				            f_{n}(x)=\ell(x) \sum_{j=0}^{n} y_{j} \frac{w_{j}}{x-x_{j}}, \text { where } \ell(x)=\prod_{i=0}^{n}\left(x-x_{i}\right) \text { and } w_{j}=\left(\ell^{\prime}\left(x_{j}\right)\right)^{-1}=\left(\prod_{j \neq i=0}^{n}\left(x_{j}-x_{i}\right)\right)^{-1} . \tag{2}
			            \end{equation*}


			      \item Show that computing $\left\{w_{j}\right\}_{j=0}^{n}$ takes $\mathcal{O}\left(n^{2}\right)$ costs but it needs to be done only once for different values $x$.

			            Hint: Unlike the expressions for $\ell_{j}(x)$, notice that $w_{j}$ only depends the nodes $\left\{x_{j}\right\}_{j=0}^{n}$.\\
			      \item To further remove the need to compute $\ell(x)$, use (2) to deduce that (1) can be rewritten as


			            \begin{equation*}
				            f_{n}(x)=\left(\sum_{j=0}^{n} y_{j} \frac{w_{j}}{x-x_{j}}\right) /\left(\sum_{j=0}^{n} \frac{w_{j}}{x-x_{j}}\right) \tag{3}
			            \end{equation*}


			            This is also called the barycentric interpolation formula for $f_{n}(x)$.\\
			            Hint: Recall $f_{n}(x)=f(x)$ for any polynomial function $f(x)$ up to degree $n$, in particular for $f(x)=1$.\\
			      \item Conclude that once $\left\{w_{j}\right\}_{j=0}^{n}$ is computed and stored, evaluating $f_{n}(x)$ using (3) takes $\mathcal{O}(n)$ costs for each $x$.\\
			      \item Implement a program to interpolate $f(x)=\sin ^{3}(\pi x)$ on $[-1,1]$ using $n=5,10,15,20$ uniformly-spaced nodes and plot resulting interpolants $f_{n}(x)$ versus $f(x)$ on 100 uniformly-spaced nodes using (3).\\
			            Hint: When evaluating $f_{n}\left(x_{i}\right)$ with (3), replace any NaN's in your output with $f\left(x_{i}\right)$.

		      \end{enumerate}
	\end{enumerate}
}
\begin{solution}
	\begin{enumerate}[label=(\alph*)]
		\item
		      \begin{enumerate}[label=(\roman*)]
			      \item First we compute the secant slopes as they will give us the coefficients for the Newton Polynomial
			            \[
				            \begin{array}{c|ccc}
					            1  & f[x_i] & f[x_i, x_{i+1}] & f[x_i, x_{i+1},x_{i+2}] \\
					            \hline
					            -1 & -5     &                 &                         \\
					               &        & 6               &                         \\
					            0  & 1      &                 & -2                      \\
					               &        & 2               &                         \\
					            1  & 3      &                 &
				            \end{array}
			            \].
			            Therefore, the corresponding Newton Polynomial is :
			            \begin{align*}
				            f_2(x) & = f[x_0]+ f[x_0,x_1](x-x_0) + f[x_0,x_1,x_2](x-x_0)(x-x_1) \\
				                   & = -5+6(x+1) -2(x+1)(x) \\
				                   & = -5 + 6x + 6 -2x^{2} - 2x \\
				                   & =  1 + 4x -2x^2
				            .\end{align*}
			      \item We update our previous secant table by adding the point:
			            \[
				            \begin{array}{c|cccc}
					            1  & f[x_i] & f[x_i, x_{i+1}] & f[x_i, x_{i+1},x_{i+2}] & f[x_i, x_{i+1},x_{i+2},x_{i+3}] \\
					            \hline
					            -1 & -5     &                 &                         &                                 \\
					               &        & 6               &                         &                                 \\
					            0  & 1      &                 & -2                      &                                 \\
					               &        & 2               &                         & 1                               \\
					            1  & 3      &                 & 1                       &                                 \\
					               &        & 4               &                         &                                 \\
					            2  & 7      &                 &                         &
				            \end{array}
			            \].
			            Therefore the corresponding Newton polynomial is,
			            \begin{align*}
				            f_3(x) & = f_2(x) + f[x_0,x_1,x_2,x_3](x-x_0)(x-x_1)(x-x_2) \\
				                   & = 1 + 4x -2x^2 + (x+1)x(x-1) \\
				                   & = 1 + 4x -2x^2 + x^3 -x \\
				                   & = 1 + 3x -2x^2 + x^3 \\
			            \end{align*}
		      \end{enumerate}
		\item \[
			      \begin{array}{c|ccccccc}
				      i & u_i & f[u_i] & f[u_i, u_{i+1}]         &                       &                               &                           &                         \\
				      \hline
				      0 & -1  & -5     &                         &                       &                               &                           &                         \\
				        &     &        & 4                       &                       &                               &                           &                         \\
				      1 & -1  & -5     &                         & \frac{6-4}{0-(-1)}=2  &                               &                           &                         \\
				        &     &        & \frac{-5-1}{-1-0}= 6    &                       & \frac{-5-2}{0-(-1)} = -7      &                           &                         \\
				      2 & 0   & 1      &                         & \frac{1-6}{0-(-1)}=-5 &                               & \frac{2-(-7)}{2-(-1)}= 3  &                         \\
				        &     &        & 1                       &                       & \frac{1 - (-5)}{2 - (-2)} = 2 &                           & \frac{9 - 3}{2 + 1} = 2 \\
				      3 & 0   & 1      &                         & \frac{3-1}{2-0}=1     &                               & \frac{29-2}{2 - (-1)} = 9 &                         \\
				        &     &        & \frac{7 - 1}{2 - 0} = 3 &                       & \frac{59 - 1}{2 - 0}= 29      &                           &                         \\
				      4 & 2   & 7      &                         & \frac{121-3}{2-0}=59  &                               &                           &                         \\
				        &     &        & 121                     &                       &                               &                           &                         \\
				      5 & 2   & 7      &                         &                       &                               &                           &
			      \end{array}
		      \]
		      Therefore, the corresponding Hermite polynomial is
		      \[
			      f(x) = -5 + 4(x+1) + 2(x+1)^2-7x(x+1)^2 + 3 x^2(x+1)^2 + 2x^2(x+1)^2(x-2)
			      .\]
		      Verification,
		      \begin{align*}
			      H_5(-1) & = -5 \qquad \checkmark \\
			      H_5(0)  & = -5 + 4 +2 = 1 \qquad \checkmark \\
			      H_5(2)  & = -5 + 12 +18 - 126+108 = 7 \qquad \checkmark \\
		      \end{align*}
		      The polynomial does interpolate the data.
		\item 	\phantom{}\\
		      \begin{enumerate}[label=(\roman*)]
			      \item \phantom{}\\
			            \begin{algorithm}[H]
				            \Fn{\FuncSty{NaivePolynomialEval(}{$x$,$\bm x$,$y$}\FuncSty{)}}{
					            \KwIn{$x$: Point of evaluation\\$\bm x$: The array of coefficients \\$\bm y$ : The array of function values}
					            \KwOut{\\sum: Polynomial evaluation}
					            \SetAlgoLined
					            \SetNoFillComment
					            \vspace{3mm}
					            s = 0\\
					            n = len(a)\\
					            \For{i=0 \dots n}{
						            $l_i$=1\\
						            \For{j= 1\dots n}{
							            \If{$i \neq j$}{
								            $l_i = l_{i} \cdot  \frac{x- x_{j}}{x_{i} - x_{j}}$\\
							            }
						            }
						            sum = $y_i \cdot l_{i}$
					            }
					            return sum
					            \caption{Naive Polynomial Interpolation}
				            }
			            \end{algorithm}
			            From the above we can see that it takes $\mathcal{O}\left( n^2 \right) $ to evaluate a polynomial. Each for loop has nearly $n$ operators which would scale like $\mathcal{O}(n^2)$.
			      \item \begin{align*}
				            f_{n}(x) & = \sum_{j=0}^{n} y_j \ell_j(x) \\
				                     & = \sum_{j=0}^{n} y_j \frac{\prod_{j\neq i=0}^n (x-x_{i})}{\prod_{j\neq i=0}^n (x_{j}-x_{i})} \\
				                     & = \sum_{j=0}^{n} y_j \frac{\prod_{i=0}^n (x-x_{i})}{(x-x_{j})\prod_{j\neq i=0}^n (x_{j}-x_{i})} \\
				                     & = \ell(x) \sum_{j=0}^{n} \frac{y_j}{(x-x_{j})\prod_{j\neq i=0}^n (x_{j}-x_{i})} \\
				                     & = \ell(x) \sum_{j=0}^{n} y_j \frac{w_j}{(x-x_{j})} \\
			            \end{align*}
			      \item  	\phantom{}\\
			            \begin{algorithm}[H]
				            \Fn{\FuncSty{computeLagrangeWeights(}{$\bm x$}\FuncSty{)}}{
					            \KwIn{\\$\bm x$: The array of coefficients }
					            \KwOut{\\sum: Polynomial evaluation}
					            \SetAlgoLined
					            \SetNoFillComment
					            \vspace{3mm}
					            s = 0\\
					            n = len(a)\\
					            w = zeros(n)
					            \For{j=0 \dots n}{
						            $w_i$=1\\
						            \For{j= 1\dots n}{
							            \If{$i \neq j$}{
								            $w_i = w_{i} \cdot  \frac{1}{x_{i} - x_{j}}$\\
							            }
						            }
					            }
					            return w
					            \caption{Computing Lagrange Polynomial weights}
				            }
			            \end{algorithm}
			            From the above we can see that it takes $\mathcal{O}\left( n^2 \right) $ to evaluate a polynomial. Each for loop has nearly $n$ operations which would scale like $\mathcal{O}(n^2)$. But it needs to be done only once because $w_j$ is independent of the input point $x$.
			      \item The interpolant is exact for $f(x)=1$, therfore, if $f(x) = 1 \implies y_{i} = 1$ and we get,
			            \begin{align*}
				            1 = f\left(x \right) = f_n(x) & = \ell(x) \sum_{j=0}^{n} y_j \frac{w_j}{(x-x_{j})} \\
				            1                             & =  \ell(x) \sum_{j=0}^{n} \frac{w_j}{(x-x_{j})} \\
				            \ell(x) = \frac{1}{\displaystyle \sum_{j=0}^{n} \frac{w_{j}}{x-x_{j}}}
				            .\end{align*}
			            Substituting in the original equation gives,
			            \[
				            f_n(x) = \frac{\displaystyle \sum_{j=0}^{n} y_j \frac{w_j}{(x-x_{j})}}{{\displaystyle \sum_{j=0}^{n} \frac{w_{j}}{x-x_{j}}}
				            }
				            .\]
			            Which is the desired result.
			      \item If we have $\left\{w_j\right\}_{j=0}^n$, then computing  $f_n(x) = \frac{\displaystyle \sum_{j=0}^{n} y_j \frac{w_j}{(x-x_{j})}}{{\displaystyle \sum_{j=0}^{n} \frac{w_{j}}{x-x_{j}}}}$ has $ 2n$ subtractions and $3n$ multiplications which are $\mathcal{O}(n)$ operations.

			      \item In jupyter notebook
		      \end{enumerate}
	\end{enumerate}
\end{solution}
\qs{Spline Interpolation}{

	\begin{enumerate}[label=(\alph*)]
		\item Compute the constant, linear, natural cubic spline by hand for the data $\{(1,2),(2,3),(3,5)\}$.\\
		\item Implement an $\mathcal{O}(n)$ program to interpolate $f(x)=\cos \left(x^{2}\right)$ with a clamped cubic spline $S(x)$ using $n=5,10,15,20$ uniformly-spaced nodes on $[0,2 \sqrt{\pi}]$ and plot resulting interpolants $S_{n} \overline{(x)}$ versus $f(x)$ on 100 uniformly-spaced nodes. Hint: Use your $\mathcal{O}(n)$ algorithm from $\mathbf{A 0}$ to solve the tridiagonal system for the coefficients $c_{i}$ and use Horner's method to evaluate the piecewise cubic polynomials for plotting.\\
		\item Consider interpolating $f$ using a not-a-knot cubic spline on $a=x_{0}<x_{1}<\cdots<x_{n-1}<x_{n}=b$ (not necessarily uniformly spaced). Write down the linear system for determining the coefficients $c_{i}$ in the form

		      $$
			      A \boldsymbol{c}=\boldsymbol{f}
		      $$

		      where $A$ is a tridiagonal matrix of $(n-1) \times(n-1), \boldsymbol{c} \in \mathbb{R}^{n-1}$ is the vector of $c_{i}$ coefficients, and $\boldsymbol{f} \in \mathbb{R}^{n-1}$ is the vector involving divided difference of $f$.\\
		      Hint: Show that the not-a-knot boundary condition implies $d_{0}=d_{1}$ and $d_{n-1}=d_{n-2}$. Use these relations to deduce relations for $c_{0}$ and $c_{n}$ by substituting them into the boxed formula on Lecture 15-page 12.

	\end{enumerate}
}
\begin{solution}
	\begin{enumerate}[label=(\alph*)]
		\item The constant spline is,
		      \[
			      S\left( x \right)  = \begin{cases}
				      2 , \quad x\in[1,2) \\
				      3,\quad x\in[2,3)   \\
				      5,\quad x= 3
			      \end{cases}
			      .\]
		      The linear spline is,
		      \[
			      S\left( x \right)  =  \begin{cases}
				      y_0+ f[x_0,x_1](x-1), \quad x\in[1,2) \\
				      y_1 + f[x_1,x_2](x-1),\quad x\in[2,3] \\
			      \end{cases}
			      = \begin{cases}
				      2+ (x-1) , \quad x\in[1,2)   \\
				      3 + 2(x-2) , \quad x\in[2,3] \\
			      \end{cases}
			      .\]
		      The cubic spline has the following form ,
		      \[
			      S(x) = \begin{cases}
				      a_{i}+b_{i}(x-x_{i}) + c_{i}(x-x_{i})^{2} + d_{i}(x-1)^3 , \quad x\in[x_{i},x_{i+1})            \\
				      a_{n-1}+b_{n-1}(x-x_{n-1})+c_{n-1}(x-x_{n-1})^2+d_{n-1}(x-x_{n-1})^3, \quad x\in[x_{n-1},x_{n}] \\
			      \end{cases}
			      .\]
		      To solve for the natural cubic spline we have to solve the following equation first,
		      \begin{align*}
			      \begin{pmatrix}
				      2(h_0+h_1) \\
			      \end{pmatrix} \begin{pmatrix} c_1  \end{pmatrix} & =  3\left( f[x_1,x_2] - f[x_0,x_1] \right) \\
			      \begin{pmatrix}
				      4 \\
			      \end{pmatrix} \begin{pmatrix} c_1  \end{pmatrix} & =  3\left( f[x_1,x_2] - f[x_0,x_1] \right) \\
			      4c_1                                             & =   3(2-1) \\
			      c_1                                              & =   \frac{3}{4}
			      .\end{align*}
		      So we have, $c_0 =c_2 =0$ and $c_1=\frac{3}{4}$.
		      The corresponding $d_i$ are,
		      \[
			      d_0 = \frac{c_1-c_0}{3h_{0}} = \frac{\frac{3}{4}}{3} = \frac{1}{4}
		      \]
		      \[
			      d_1 = \frac{c_2-c_1}{3h_{1}} = \frac{-\frac{3}{4}}{1} = -\frac{1}{4}
			      .\]
		      Next we compute the $b_{i}$,
		      \[
			      b_{0} = f[x_{0},x_1] - \frac{h_{0}}{3} \left( c_1+2c_0 \right) = 1- \frac{1}{3} \left(\frac{3}{4}  \right) =\frac{3}{4}
		      \]
		      \[
			      b_{1} = f[x_{1},x_2] - \frac{h_{1}}{3} \left( c_2+2c_1 \right) = 2- \frac{1}{3} \left(\frac{6}{4}  \right) =\frac{3}{2}
			      .    \]
		      So the corresponding natural cubic spline is,
		      \[
			      S(x)=  \begin{cases}
				      2+\frac{3}{4}(x-1) + \frac{3}{4}(x-1)^3 , \quad x\in[1,2)                 \\
				      3+\frac{3}{2}(x-2)+\frac{3}{4}(x-2)^2-\frac{1}{4}(x-2)^3, \quad x\in[2,3] \\
			      \end{cases}
			      .\]
		\item In jupyter notebook.
		\item The \emph{\bfseries not-a-knot} cubic spline condition is,
		      \[
			      p_0'''(x_1) = p_1'''(x_1) \quad \text{and} \quad p_{n-2}'''(x_{n-1}) = p_{n-1}'''(x_{n-1})
		      \]
		      \[
			      p_0'''(x_0) = d_0 = d_1 = p_1'''(x_1) \quad \text{and} \quad p_{n-1}'''(x_{n-1}) = d_{n-1} = d_{n-2} = p_{n-2}''' (x-1)
			      . \]
		      Using the above in the relations derived between $c_{i}$ and $d_{i}$, we get
		      \[
			      d_0 = \frac{c_1-c_0}{3h_{0}} = \frac{c_{2}-c_{1}}{3h_{1}} = d_{1}
			      .\]
		      Simplifying gives,
		      \[
			      -c_{0}h_1 + (h_0+h_1)c_{1} -c_{2}h_{0} = 0
			      .\]
		      We also have,
		      \[
			      c_0h_0+ 2(h_0 + h_1)c_1 + h_1c_2 = 3(
			      .\]
		      Eliminating $c_0$ gives,
		      \[
			      (\frac{h_0^2}{h_1} + 3h_0 + h_1) c_1 + (h_1-\frac{h_0^2}{h_1})c_2 = 3(f[x_1,x_2] - f[x_0,x_1])
			      .\]
		      On the other hand,
		      \[
			      d_{n-1} = \frac{c_{n} - c_{n-1}}{3h_{n-1}} = \frac{c_{n-2}-c_{n-1}}{3h_{n-2}} = d_{n-2}
			      .\]
		      Simplifying the above gives,
		      \[
			      -c_{n-2}h_{n-1} + (h_{n-1}+h_{n-2})c_{n-1} -c_{n}h_{n-2} = 0
			      .\]
		      Similarly as before, we get
		      \[
			      (\frac{h_{n-2}^2}{h_{n-1}} + 3h_{n-2} + h_{n-1}) c_{n-1} + (-h_{n-1}+\frac{h_{n-2}^2}{h_{n-1}})c_{n-1} = 3(f[x_1,x_2] - f[x_0,x_1])
			      .\]
		      Which gives the following linear system,
		      \[
			      A = \begin{pmatrix}
				       & \frac{h_0^2}{h_1} + 3h_0 + h_1 & h_1-\frac{h_0^2}{h_1}              &                                                & \\
				       & \ddots                         & \ddots                             & \ddots                                         & \\
				       &                                & -h_{n-1}+\frac{h_{n-2}^2}{h_{n-1}} & \frac{h_{n-2}^2}{h_{n-1}} + 3h_{n-2} + h_{n-1} & \\
			      \end{pmatrix}
			      .\]
		      \[
			      \vec{{c}} =  \begin{pmatrix} c_1\\ \vdots\\ c_{n-1} \end{pmatrix}
			      .\]
		      \[
			      \bm f = \begin{pmatrix} 3(f[x_1,x_2] - f[x_0,x_1])\\ \vdots\\ 3\left( f[x_{n-2}, x_{n-1}] - f[x_{n-3},x_{n-2}] \right)  \end{pmatrix}
		      \]
	\end{enumerate}
\end{solution}
\qs{Trigonometric interpolation and Discrete Fourier Transform}{
	Denote the vectors $\boldsymbol{f}=\left(f_{0}, f_{1}, \ldots, f_{N-1}\right)^{T}$ and $\hat{\boldsymbol{f}}=\left(\hat{f}_{0}, \hat{f}_{1}, \ldots, \hat{f}_{N-1}\right)^{T}$. Recall that the Discrete Fourier Transform (DFT) of $\boldsymbol{f}$ is $\hat{\boldsymbol{f}}$ and the inverse Discrete Fourier Transform of $\hat{\boldsymbol{f}}$ is $\boldsymbol{f}$ given by in components


	\begin{equation*}
		\hat{f}_{k}=\sum_{j=0}^{N-1} f_{j} \omega_{N}^{-j k}, \quad f_{j}=\frac{1}{N} \sum_{k=0}^{N-1} \hat{f}_{k} \omega_{N}^{j k}, \quad \text { where } \omega_{N}:=e^{\frac{2 \pi i}{N}}, \text { for } 0 \leq k \leq N-1 \tag{4}
	\end{equation*}
	\begin{enumerate}[label=(\alph*)]

		\item Compute explicitly the DFT $\hat{\boldsymbol{f}}$ of $\boldsymbol{f}=(5,1,5,1,5,1,5,1)^{T}$ using (4). Sketch the associated truncated Fourier series $f_{N}(x)$ on $[0,2 \pi]$ ?\\
		\item Set all frequency components $\hat{f}_{4}, \ldots, \hat{f}_{7}$ of $\hat{\boldsymbol{f}}$ to zero and call this vector $\tilde{\boldsymbol{f}}$. Now compute the inverse DFT of $\tilde{\boldsymbol{f}}$ and sketch the new truncated Fourier series $\tilde{f}_{N}(x)$ ? Explain why this result is expected.\\
		      Remark: This is an example of a "low-pass" filter and has applications in signal processing.\\
		\item The following exercise outlines the main idea behind the Fast Fourier Transform (FFT) algorithm introduced by Cooley and Tukey in 1965. For simplicity, we assume $N=2^{m}$ for some integer $m$.\\
		      \begin{enumerate}[label=(\roman*)]

			      \item For $0 \leq k \leq N-1$, show that the sum in (4) of the DFT can be written in terms of even/odd components as

			            $$
				            \hat{f}_{k}=\sum_{j=0}^{\frac{N}{2}-1} f_{2 j}\left(\omega_{\frac{N}{2}}\right)^{-j k}+\omega_{N}^{-k} \sum_{j=0}^{\frac{N}{2}-1} f_{2 j+1}\left(\omega_{\frac{N}{2}}\right)^{-j k}
			            $$

			            Hint: Express $\omega_{\frac{N}{2}}$ in terms of $\omega_{N}$.\\
			      \item Denote $F F T(\boldsymbol{f}, N)$ as the DFT of $\boldsymbol{f}$ with $N$ components and define $\boldsymbol{\omega}:=\left(1, \omega_{N}^{-1}, \omega_{N}^{-2} \ldots \omega_{N}^{-(N-1)}\right)^{T}$. By writing out the $N$ equations from part (i), deduce that $\operatorname{FFT}(\boldsymbol{f}, N)$ can be written recursively as

			            $$
				            F F T(\boldsymbol{f}, N)=\binom{F F T\left(\boldsymbol{f}_{\text {even }}, \frac{N}{2}\right)}{F F T\left(\boldsymbol{f}_{\text {even }}, \frac{N}{2}\right)}+\boldsymbol{\omega} \odot\binom{F F T\left(\boldsymbol{f}_{\text {odd }}, \frac{N}{2}\right)}{F F T\left(\boldsymbol{f}_{\text {odd }}, \frac{N}{2}\right)}
			            $$

			            where $\odot$ denotes element-wise multiplication and $\boldsymbol{f}_{\text {even }} / \boldsymbol{f}_{\text {odd }}$ denotes the even/odd components of $\boldsymbol{f}$. Using this, write a recursive pseudocode for the function $F F T$ to compute the DFT of $\boldsymbol{f}$.\\
			      \item Let $T(N)$ be the FLOPs to compute $F F T(\boldsymbol{f}, N)$ and assume $T(1)$ is a constant $C$. Express $T(N)$ as a recursive function of $T(N / 2)$ and show that the overall computational cost to compute $F F T(\boldsymbol{f}, N)$ is $\mathcal{O}\left(N \log _{2} N\right)$. How much faster is FFT compared to DFT using direct matrix-to-vector multiplication for $m=5,10,15,20$ ?\\
			            Remark: FFT was considered one of top 10 most impactful algorithm of $20^{\text {th }}$ century - see https: // archive. siam. org/pdf/news/637. pdf.
		      \end{enumerate}
	\end{enumerate}
}
\begin{solution}
	\begin{enumerate}[label=(\alph*)]
		\item  Given that $\bm f = \left(5,1,5,1,5,1,5,1\right) $ we want to compute $\hat{\bm f} $
		      \begin{align*}
			      \hat{f}_0 & = \sum_{j=0}^{7} f_{j} \left(e^{\frac{2\pi i}{7}}\right)^{0} = 24 \\
			      \hat{f}_1 & = \sum_{j=0}^{7} f_{j} \left(e^{\frac{2\pi i}{8}}\right)^{-j} \\
			                & =   5 + e^{\frac{-2\pi i}{8}} + 5e^{\frac{-4\pi i }{8}} + e^{\frac{-6\pi i }{8}} +5 e^{\frac{-8\pi i }{8}}
			      + e^{\frac{-10\pi i}{8}} + 5e^{\frac{-12\pi i}{8}} + e^{\frac{-14\pi i}{8}} \\
			                & =  5 + e^{\frac{-\pi i}{4}} + 5 e^{\frac{-\pi i }{2}} + e^{\frac{-3\pi i }{4}} -5
			      - e^{\frac{-\pi i}{4}} - 5e^{\frac{-\pi i}{2}} - e^{\frac{-3\pi i}{4}} \\
			                & = 0 \\
			      \hat{f}_2 & = \sum_{j=0}^{7} f_{j} \left(e^{\frac{2\pi i}{8}}\right)^{-2j} \\
			                & =   5 + e^{\frac{-4\pi i}{8}} + 5e^{\frac{-8\pi i }{8}} + e^{\frac{-12\pi i }{8}} +5 e^{\frac{-16\pi i }{8}}
			      + e^{\frac{-20\pi i}{8}} + 5e^{\frac{-24\pi i}{8}} + e^{\frac{-28\pi i}{8}} \\
			                & =  5 + e^{\frac{-\pi i}{2}} + 5 e^{-\pi i } + e^{\frac{-3\pi i }{2}} +5
			      + e^{\frac{-5\pi i}{2}} + 5e^{-3\pi i} + e^{\frac{-7\pi i}{2}} \\
			                & =  5 + e^{\frac{-\pi i}{2}} - 5  - e^{\frac{-\pi i }{2}} +5
			      + e^{\frac{-\pi i}{2}} - 5 - e^{\frac{-\pi i}{2}} \\
			                & = 0 \\
			      \hat{f}_3 & = \sum_{j=0}^{7} f_{j} \left(e^{\frac{2\pi i}{8}}\right)^{-3j} \\
			                & =   5 + e^{\frac{-3\pi i}{4}} + 5e^{\frac{-6\pi i }{4}} + e^{\frac{-9\pi i }{4}} +5 e^{\frac{-12\pi i }{4}}
			      + e^{\frac{-15\pi i}{4}} + 5e^{\frac{-18\pi i}{4}} + e^{\frac{-21\pi i}{4}} \\
			                & =  5 + e^{\frac{-3\pi i}{4}} - 5e^{\frac{-\pi i }{2}} + e^{\frac{-\pi i }{4}} -5
			      - e^{\frac{-3\pi i}{4}} + 5e^{\frac{-\pi i}{2}} - e^{\frac{-\pi i}{4}} \\
			                & =  0 \\
			      \hat{f}_4 & = \sum_{j=0}^{7} f_{j} \left(e^{\frac{2\pi i}{8}}\right)^{-4j} \\
			                & =   5 + e^{1\pi i} + 5e^{2\pi i} + e^{3\pi i} +5 e^{4\pi i}
			      + e^{5\pi i} + 5e^{6\pi i} + e^{7\pi i} \\
			                & =  5 - 1 + 5 - 1 +5 -1  + 5 -1 \\
			                & =  16 \\
			      \hat{f}_5 & = \sum_{j=0}^{7} f_{j} \left(e^{\frac{2\pi i}{8}}\right)^{-5j} \\
			                & =   5 + e^{\frac{-5\pi i}{4}} + 5e^{\frac{-5\pi i }{2}} + e^{\frac{-15\pi i }{4}} +5 e^{-5\pi i }
			      + e^{\frac{-25\pi i}{4}} + 5e^{\frac{-15\pi i}{2}} + e^{\frac{-35\pi i}{4}} \\
			                & =  5 - e^{\frac{-\pi i}{4}} + 5e^{\frac{-\pi i }{2}} + e^{\frac{-\pi i }{4}} -5
			      - e^{\frac{-3\pi i}{4}} + 5e^{\frac{-\pi i}{2}} - e^{\frac{-\pi i}{4}} \\
			                & =  0 \\
			      \hat{f}_6 & = \sum_{j=0}^{7} f_{j} \left(e^{\frac{2\pi i}{8}}\right)^{-6j} \\
			                & =   5 + e^{-3\pi i} + 5e^{-3\pi i } + e^{\frac{-9\pi i }{2}} +5 e^{-6\pi i }
			      + e^{\frac{-15\pi i}{2}} + 5e^{-9\pi i} + e^{\frac{-21\pi i}{2}} \\
			                & =  0 \\
		      \end{align*}
		      \begin{align*}
			      \hat{f}_7 & = \sum_{j=0}^{7} f_{j} \left(e^{\frac{2\pi i}{8}}\right)^{-7j} \\
			                & =   5 + e^{\frac{-7\pi i}{2}} + 5e^{-\frac{21\pi i}{4} } + e^{-7\pi i } +5 e^{-6\pi i }
			      + e^{\frac{-15\pi i}{2}} + 5e^{-9\pi i} + e^{\frac{-21\pi i}{2}} \\
			                & =  5 - e^{\frac{-\pi i}{4}} + 5e^{\frac{-\pi i }{2}} + e^{\frac{-\pi i }{4}} -5
			      - e^{\frac{-3\pi i}{4}} + 5e^{\frac{-\pi i}{2}} - e^{\frac{-\pi i}{4}} \\
			                & =  0 \\
		      \end{align*}
		\item We found that $\hat{ \bm f }= \left(24,0,0,16,0,0,0,0\right) $, now we set $f_4, \dots, f_7$, which gives $\tilde{\bm f} = \left( 24,0,0,0,0,0,0,0 \right)  $
		      \begin{align*}
			      \tilde{f}_0 & = \frac{1}{8}\sum_{j=0}^{7} f_{j} \left(e^{\frac{2\pi i}{8}}\right)^{0}  = 3 \\
			      \tilde{f}_1 & = \frac{1}{8}\sum_{j=0}^{7} f_{j} \left(e^{\frac{2\pi i}{8}}\right)^{-1}=   3 \\
			      \tilde{f}_2 & =\frac{1}{8}\sum_{j=0}^{7} f_{j} \left(e^{\frac{2\pi i}{8}}\right)^{-2} =   3 \\
			      \tilde{f}_3 & =\frac{1}{8}\sum_{j=0}^{7} f_{j} \left(e^{\frac{2\pi i}{8}}\right)^{-3} =   3 \\
			      \tilde{f}_4 & =\frac{1}{8}\sum_{j=0}^{7} f_{j} \left(e^{\frac{2\pi i}{8}}\right)^{-4} =   3 \\
			      \tilde{f}_5 & =\frac{1}{8}\sum_{j=0}^{7} f_{j} \left(e^{\frac{2\pi i}{8}}\right)^{-5} =   3 \\
			      \tilde{f}_6 & =\frac{1}{8}\sum_{j=0}^{7} f_{j} \left(e^{\frac{2\pi i}{8}}\right)^{-6} =   3 \\
			      \tilde{f}_7 & =\frac{1}{8}\sum_{j=0}^{7} f_{j} \left(e^{\frac{2\pi i}{8}}\right)^{-7} =   3 \\
		      \end{align*}
		\item \begin{enumerate}[label=(\roman*)]
			      \item  We have,
			            \begin{align*}
				             & \hat{f}_k=\sum_{j=0}^{N-1} f_j \omega_{N}^{-j k} \quad N=2^m \\
				             & \hat{f}_k=\sum_{j=0}^{\frac{N}{2}-1} f_{2 j} \omega_N^{-2 j k}+\sum_{j=0}^{\frac{N}{2}-1} f_{2 j+1}\omega_N^{(-2j-1) k} \\
				             & =\sum_{j=0}^{\frac{N}{2}-1} f_{2 j} e^{-(2 \pi i /2^m){2j k}}+\sum_{j=0}^{\frac{N}{2}-1} f_{2 j+1} e^{-\frac{2 \pi ijk }{2^m}} \cdot e^{-\frac{2 \pi ik}{2^m}} \\
				             & =\sum_{j=0}^{\frac{N}{2}-1} f_{2 j} e^{-(\frac{2 \pi i }{\frac{N}{2}}){j k}}+\sum_{j=0}^{\frac{N}{2}-1} f_{2 j+1} e^{-\frac{2 \pi ijk }{\frac{N}{2}}} \cdot e^{-\frac{2 \pi ik}{N}} \\
				             & =\sum_{j=0}^{\frac{N}{2}-1} f_{2 j} \omega_{\frac{N}{2}}^{-j k}+\omega_{N}^{-k} \sum_{j=0}^{\frac{N}{2}-1} f_{2 j+1} \omega_{\frac{N}{2}}^{-jk} \\
				             & \text { is the desired result. }
			            \end{align*}
			      \item
			            $$
				            \hat{f}_k=\sum_{j=0}^{\frac{N}{2}-1} f_{2 j}\left(\omega_{\frac{N}{2}}\right)^{-j k}+\omega_{N}^{-k} \sum_{j=0}^{\frac{N}{2}-1} f_{2 j+1}\left(\omega_{\frac{N}{2}}\right)^{-j k}
			            $$
			            For the first $\frac{N}{2}$ entries
			            $ f_{\text{even}}$ \text{ is s.t } $f_{2 j}$ maps to $f_{\text {even}_j}$ \text{and} $f_{\text{odd}}$ is s.t $f_{2j+1}$ maps to $f_{\text{odd}_j}$\text{ then first half of $f$ can be written as}

			            \begin{align*}
				             & \quad \operatorname{FFT}\left(f_{\text {even }}, N / 2\right)+\vec{\alpha}\  {\odot}\  \operatorname{FFT}\left(f_{\text {odd }}, N / 2\right), \text{where } \vec{\alpha}=\left(1, \omega^{-1}, \ldots . \omega^{-\left(\frac{N}{2}-1\right)}\right)
			            \end{align*}
			            For the next $\frac{N}{2}$ entries, take $k<N / 2$
			            \begin{align*}
				             & \therefore \hat{f}_{k+\frac{N}{2}}=\sum_{j=0}^{\frac{N}{2}-1} f_{\text {even}_j} \omega_{\frac{N}{2}}^{-j(k+N / 2)}+\omega_{N}^{-(k+N / 2)} \sum_{j=0}^{\frac{N}{2}-1} f_{\text {odd}_j}\left(\omega_{\frac{N}{2}}^{-j\left(k+\frac{N}{2}\right)}\right) \\
				             & =\sum_{j=0}^{N / 2-1} f_{\text {even}_j} \cdot \omega_{N / 2}^{-j k} \cdot \omega_{N / 2}^{-j N / 2}+\omega_{N}^{-\left( k+\frac{N}{2} \right) } \sum_{j=0}^{\frac{N}{2}-1} f_{\text {odd}_j} \cdot \omega_{\frac{N}{2}}^{-j k} \omega_{N / 2}^{-j N / 2} \\
				             & =\sum_{j=0}^{N / 2-1} f_{\text{even}_j} \omega_{N / 2}^{-j k}+\omega_N^{-(k+\frac{N}{2} )} \sum_{j=0}^{\frac{N}{2}-1} f_{\text {odd}_j} \omega_{\frac{N}{2}}^{-jk} \\
				             & =F F T\left(f_{\text {even }}, \frac{N}{2} \right)+\vec{\beta} \odot F F T\left(f_{\text {odd }}, \frac{N}{2}\right) \\
			            \end{align*}
			            where $\beta = \left( \omega^{-\frac{N}{2}},\dots, \omega^{-N} \right) $,
			            $\vec{\omega}=\overrightarrow{\alpha} \mid \vec{\beta}$ \quad \text { where ' } '|' stands for   concatenation.
			      \item We have : $T(N) = 2T(\frac{N}{2}) + \mathcal{O}(N)$, because for $N$ we have to compute FFT for $2$, $\frac{N}{2}$ quantities. Therefore,
			            \[
				            \begin{aligned}
					            T(N)                             & = 2T\!\Bigl(\frac{N}{2}\Bigr) + \mathcal{O}(N), \\
					            T\!\Bigl(\frac{N}{2}\Bigr)       & = 2T\!\Bigl(\frac{N}{4}\Bigr) + \mathcal{O}(N), \\
					            T\!\Bigl(\frac{N}{4}\Bigr)       & = 2T\!\Bigl(\frac{N}{8}\Bigr)+\mathcal{O}(N)  , \\
					                                             & \ \,\vdots                                      \\
					            T\!\Bigl(\frac{N}{2^{m-1}}\Bigr) & = 2T\!\Bigl(\frac{N}{2^m}\Bigr)+\mathcal{O}(N).
				            \end{aligned}
			            \]
			            There $\log_2 N$ equations above, so we have to do $T(f(n))$ amount of work $\log_2 N$ times.
			            Notice that \(N = 2^m\). Hence, \(\frac{N}{2^m} = 1\). We have,
			            \[
				            T\!\Bigl(\frac{N}{2^{m-1}}\Bigr) = 2T(1).
			            \]

			            Rewriting:
			            \[
				            \begin{aligned}
					            T(N) \;                             & -\; 2\,T\!\Bigl(\frac{N}{2}\Bigr)   & =                 & \; \mathcal{O}(N), \\
					            T\!\Bigl(\frac{N}{2}\Bigr) \;       & -\; 2\,T\!\Bigl(\frac{N}{4}\Bigr)   & =                 & \; \mathcal{O}(N)
					            ,                                                                                                                  \\
					            T\!\Bigl(\frac{N}{4}\Bigr) \;       & -\; 2\,T\!\Bigl(\frac{N}{8}\Bigr)   & =                 & \; \mathcal{O}(N), \\
					                                                & \quad \vdots                        &                   &                    \\
					            T\!\Bigl(\frac{N}{2^{m-1}}\Bigr) \; & -\; 2\,T\!\Bigl(\frac{N}{2^m}\Bigr)
					                                                & =                                   & \; \mathcal{O}(N)
				            \end{aligned}
			            \]

			            When we add up all these equations (telescoping sum), almost all terms cancel, leaving:
			            \[
				            T(N) - 2^m T(1) = \mathcal{O}(N) \sum_{i=1}^{\log N} 1  = \mathcal{O}(N\log_2 N)
			            \]
			            Therfore,
			            \[
				            T(N) = N + \mathcal{O}(N\log_2 N)
				            .\]
			            \[
				            T(N) = O(N \log N).
			            \]
			            For $m = 5,10,15,20$
			            \begin{align*}
				            R_5    & = \frac{2^5}{5} \text{times faster } \\
				            R_{10} & = \frac{2^{10}}{10} \text{times faster } \\
				            R_{15} & = \frac{2^{15}}{15} \text{times faster } \\
				            R_{20} & = \frac{2^{20}}{20} \text{times faster } \\
			            \end{align*}
		      \end{enumerate}

	\end{enumerate}
\end{solution}

\qs{Numerical Differentiation and Integration}{
	\begin{enumerate}[label=(\alph*)]
		\item In solving boundary value problems with Neumann boundary conditions, derivative values at the boundary need to be approximated from one side of the boundary. For some constants $a, b, c$, design an one-sided finite difference approximation for $f^{\prime}(x)$ of the form,

		      $$
			      D_{h} f(x)=af(x)+bf(x+h)+cf(x+2 h),
		      $$

		      with the highest degree of accuracy. Derive its error term and find the highest $p$ so that the error is bounded by $\mathcal{O}\left(h^{p}\right)$.\\
		\item Consider integrating $f(x)=e^{-x^{2}}$ which often arises in probability and statistics for computing the cumulative distribution function of the Gaussian distribution.\\
		      \begin{enumerate}[label=(\roman*)]
			      \item Write down the quadrature formulas involving $h$ and $n$ for $I(f)=\int_{a}^{b} f(x) d x$ using the composite midpoint, trapezoidal and Simpson's method. State their degree of accuracy.\\
			      \item Recall the error for each of the composite rules is bounded by $\frac{M_{2}}{24}(b-a) h^{2}, \frac{M_{2}}{12}(b-a) h^{2}$, and $\frac{M_{4}}{180}(b-a) h^{4}$ respectively, where $M_{n}:=\max _{x \in[a, b]}\left|f^{(n)}(x)\right|$. Estimate the number of subintervals $n$ needed for each method in order to guarantee an error tolerance of $10^{-6}$ for $I(f)$ on $[0,1]$. How do these estimates compare to part (iv)?\\
			      \item The 4-point Gauss Quadrature has Gauss points on $\left\{ \pm \sqrt{\frac{3}{7}-\frac{2}{7} \sqrt{\frac{6}{5}}}, \pm \sqrt{\frac{3}{7}+\frac{2}{7} \sqrt{\frac{6}{5}}}\right\}$ with respective weights $\left\{\frac{18+\sqrt{30}}{36}, \frac{18+\sqrt{30}}{36}, \frac{18-\sqrt{30}}{36}, \frac{18-\sqrt{30}}{36}\right\}$. Write down the 4 -point Gauss Quadrature of $I(f)$ on $[0,1]$.\\
			      \item Using parts (i)-(iii), implement a program to generate a table of values for the four quadrature methods to reach the tolerance criterion of $10^{-5}$. Using the exact value of $I(f)$ on $[0,1]$ is $0.7468241328 \ldots$, verify the three composite methods have the expected error of order $\mathcal{O}\left(h^{p}\right)$, i.e. graph a log-log plot of error versus $h$ to verify the order $p$. Rank their computational costs based on their number of function evaluations.\\
		      \end{enumerate}
		\item Recall the composite Trapezoidal rule has the error $I(f)=I_{h}(f)+\mathcal{O}\left(h^{2}\right)$.\\
		      \begin{enumerate}[label=(\roman*)]
			      \item Apply Richardson's extrapolation to the composite Trapezoidal rule to derive a more accurate quadrature formula. What is the expected improved order?\\
			      \item Demonstrate the improved convergence with your new quadrature rule for the integral from part (b) by graphing a log-log plot of error versus $h$ to verify the order $p$. Explain briefly if you do not get the expected order.
		      \end{enumerate}

	\end{enumerate}
}
\begin{solution}
	\begin{enumerate}[label=(\alph*)]
		\item Because we have three unknowns $a,b,c$ we can have 3 equations and hence highest degree of accuracy we can achieve is 2. Therefore consider,

		      $	\text{If } f(x)=1$,
		      \[
			      D_h f(x) = f'(x) = 0= a +b +c
			      .\]
		      If $f(x) = x$
		      \[
			      D_h f(x)= f'(x) = 1 = ax + b(x+h)+c(x+2h)
			      .\]
		      \[
			      \implies 1 = (a+b+c)x + (b+2c)h = (b+2c)h \implies \frac{1}{h} = b+2c
			      .\]
		      If $f(x) = x^2$
		      \[
			      D_h f(x) = f'(x) = 2x = ax^2 + b(x+h)^2+c(x+2h)^2
			      .\]
		      \[
			      \implies 2x = (a+b+c)x^2 + 2(b+2c)xh+ (b+4c)h^2 = 2x+(b+4c)h^2
			      .\]
		      \[
			      \implies 0 = b+4c = \frac{1}{h} + 2c \implies \frac{-1}{2h} = c
			      .\]
		      Also,
		      \[
			      0 = b+4c = \frac{2}{h} - b \implies b = \frac{2}{h}
			      .\]
		      Combining gives,
		      \[
			      a = \frac{-3}{2h}
			      .\]
		      Therefore,
		      \[
			      D_h f(x) = \frac{-3f(x) + 4f(x+h) -f(x+2h)}{2h}
			      .\]
		      Which has degree order of accuracy of 2.

		      Using the Taylor Remainder Theorem around $x$ gives,
		      \begin{align*}
			      D_{h}f(x)                       & =
			      \frac{1}{2h} \left( -3f(x) + 4f(x) + 4 f'(x) h + 2f''(x)h^2 + \frac{2f'''(\xi_1)h^3}{3} \right. \\
			                                      & \hspace{2cm} \left. - f\left( x \right) - 2f'(x)h - 2f''(x) h^2  -   \frac{4f'''(\xi_2)h^3}{3} +  \mathcal{O}(h^4) \right) \\
			                                      & = \frac{1}{2h} \left(2f'(x)h-\frac{2}{3} \left( f'''(\xi_1) - 2f'''(\xi_2)) \right) h^3 + \mathcal{O}(h^{4}) \right) \\
			      |D_{h}f\left(x \right) - f'(x)| & = \left|\frac{1}{3} (f'''(\xi_1) - 2f'''(\xi_2))h^2 + \mathcal{O}(h^{3})\right|\le \underset{\xi\in[x,x+2h]}{\operatorname{max}} f'''\left( \xi \right) h^2 + \mathcal{O}(h^3)
			      .\end{align*}
		\item
		      \begin{enumerate}[label=(\roman*)]
			      \item Doing the integral using the mid-point rule we get,
			            \[
				            I(f) \approx I_\text{mid-point}(f) = \sum_{i=1}^{n} f\left(a+ih-\frac{h}{2}\right) h
				            .\]
			            \[
				            I(f) \approx \sum_{i=1}^{n} \exp\left(\left(a+ih-\frac{h}{2}\right)^{2}\right) h
			            \]
			            Doing the integral using the trapezoidal rule we get,
			            \[
				            I(f) \approx I_\text{trapezoidal}(f) = h\left(\frac{\exp(-a^2)}{2} +  \sum_{i=1}^{n-1} \exp\left(-(a+ih)^2\right) + \frac{\exp(-b^2)}{2} \right)
				            .\]
			            Doing the integral using the simpsons rule we get,
			            \[
				            \begin{split}
					            I(f) \approx I_\text{simpson}(f) & = \frac{h}{3}\left(\frac{\exp(-a^2)}{2} + 4 \sum_{i=1}^{\frac{n}{2}} \exp\left(-(a+(2i-1)h)^2\right) \right. \\& \left.  \hspace{3cm}+2 \sum_{i=1}^{\frac{n}{2}-1} \exp\left(-(a+2ih)^2\right)  + \frac{\exp(-b^2)}{2} \right)
					            .\end{split}
			            \]
			      \item  $f(x) = e^{-x^2}$,
			            \begin{align*}
				            f'(x)               & = -2xe^{-x^2} \\
				            f''(x)              & = -2e^{-x^2} + 4x^2e^{-x^2} \\
				            f'''(x)             & = 4xe^{-x^2} +8xe^{-x^2} -8x^3e^{-x^2} = 12xe^{-x^2} - 8x^3e^{-x^2} \\
				            f''''\left(x\right) & = 12e^{-x^2} - 24x^2e^{-x^2} +16x^{4}e^{-x^2}
			            \end{align*}
			            From desmos, I saw that $M_2 = 2$
			            \begin{align*}
				            \|I_{\text{mid-point}}(f) - I(f)\| & \leq \frac{M_2}{24} h^2 + \mathcal{O}(h^3) \leq 10^{-6} \\
				            \|I_{\text{mid-point}}(f) - I(f)\| & \leq \frac{1}{12} h^2 + \mathcal{O}(h^3) \leq 10^{-6} \\
				            .\end{align*}
			            For $h=\sqrt{12}  \times 10^{-3}$ we get the desired error tolerance.
			            \begin{align*}
				            \|I_{\text{trapezoidal}}(f) - I(f)\| & \leq \frac{M_2}{12} h^2 + \mathcal{O}(h^3) \leq 10^{-6} \\
				            \|I_{\text{trapezoidal}}(f) - I(f)\| & \leq \frac{1}{6} h^2 + \mathcal{O}(h^3) \leq 10^{-6} \\
				            .\end{align*}
			            For $h= \sqrt{6}  \times 10^{-3}$ we get the desired error tolerance.
			            From desmos, I saw that $M_4 = 12$
			            \begin{align*}
				            \|I_{\text{simpson}}(f) - I(f)\| & \leq \frac{1}{15} h^4 + \mathcal{O}(h^3) \leq 10^{-6} \\
				            \|I_{\text{simpson}}(f) - I(f)\| & \leq \frac{1}{15} h^4 + \mathcal{O}(h^3) \leq 10^{-6} \\
				            .\end{align*}
			            For $ h =\sqrt[4]{15} \cdot 10^{-\frac{3}{2}}   $ we get the desired error tolerance.
			      \item  $I\left( f \right) $ can be approximated using a 4pt gauss quadrature as follows,
			            \begin{align*}
				            I(f) = \int_0^1 f(x) dx & = \frac{b-a}{2 } \int_{-1}^{1} f(\frac{b-a}{2}x + \frac{b+a}{2}) dx \\
				                                    & = \frac{1}{2} \frac{18+\sqrt{30}}{36} f\left( \frac{1}{2} \sqrt{\frac{3}{7}-\frac{2}{7} \sqrt{\frac{6}{5}}}+ \frac{1}{2}\right) +
				            \frac{1}{2} \frac{18+\sqrt{30}}{36} f\left(- \frac{1}{2} \sqrt{\frac{3}{7}-\frac{2}{7} \sqrt{\frac{6}{5}}}+ \frac{1}{2}\right) \\
				                                    & +\frac{1}{2} \frac{18-\sqrt{30}}{36} f\left( \frac{1}{2} \sqrt{\frac{3}{7}+\frac{2}{7} \sqrt{\frac{6}{5}}}+ \frac{1}{2}\right) \\
				                                    & +\frac{1}{2} \frac{18-\sqrt{30}}{36} f\left(- \frac{1}{2} \sqrt{\frac{3}{7}+\frac{2}{7} \sqrt{\frac{6}{5}}}+ \frac{1}{2}\right)
				            .\end{align*}
		      \end{enumerate}
		\item
		      \begin{enumerate}[label=(\roman*)]
			      \item Using the Richardson extrapolation theorem on, $I(f) = I_h(f) + \mathcal{O}(h^2)$ .
			            Let $I_h = I_{h,0}$
			            \[
				            I_{h,1} = \frac{2^{2} I_{\frac{h}{2},0} - I_{h,0}}{2^{2} - 1} = \frac{4 I_{\frac{h}{2},0} - I_{h,0}}{3}
				            .\]
			            The expected improved order is $3$.
			      \item In jupyter notebook.
		      \end{enumerate}
	\end{enumerate}
\end{solution}
\end{document}
