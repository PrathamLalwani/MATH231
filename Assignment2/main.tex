\documentclass{report}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{textcomp}
\usepackage[english]{babel}
\usepackage{amsmath, amssymb}
\usepackage{enumitem}
\usepackage{bm}
% figure support
\usepackage{import}
\usepackage{xifthen}
\pdfminorversion=7
\usepackage{pdfpages}
\usepackage{transparent}

% \newcommand{\incfig}[1]{%
% 	\def\svgwidth{\columnwidth}
% 	\import{./figures/}{#1.pdf_tex}
% }
\pdfsuppresswarningpagegroup=1

\input{preamble}
\input{macros}
\input{letterfonts}
\title{\Huge{MATH 231 : Numerical ODEs}}
\author{\huge{Pratham Lalwani}}
\date{\today}

\begin{document}
\maketitle
\qs{Eigenvalues of special tridiagonal matrices}{

	This question is about finding eigenvalues of tridiagonal linear systems arising from applications, specifically finding the eigenvalues of an $n \times n$ matrix of the form,

	$$
		A=\left(\begin{array}{ccccc}
			a & b      &        &        &   \\
			c & a      & b      &        &   \\
			  & \ddots & \ddots & \ddots &   \\
			  &        & c      & a      & b \\
			  &        &        & c      & a
		\end{array}\right)
	$$

	where $a, b, c$ are real numbers with $b c>0$ (i.e. $b$ and $c$ have the same signs).
	\begin{enumerate}[label=(\alph*)]
		\item Show that the eigenvalue problem of $A$ is equivalent to the equations

		      $$
			      \begin{aligned}
				      c v_{j-1}+(a-\lambda) v_j+b v_{j+1} & =0, \quad j=1, \ldots, n \\
				      v_0                                 & =0=v_{n+1}
			      \end{aligned}
		      $$

		      where $\boldsymbol{v}=\left(v_1, \ldots, v_n\right)^T$ is an eigenvector of $A$ associated with the eigenvalue $\lambda$.
		\item The recurrence relation (1) is a second order linear difference equation and can be solved similar to second order linear differential equations. By guessing $v_j=r^j$ for some constant $r$, show that $r$ satisfies

		      $$
			      r_{ \pm}=\frac{\lambda-a \pm \sqrt{(\lambda-a)^2-4 b c}}{2 b}, \quad \text { with } \quad r_{+} r_{-}=\frac{c}{b}
		      $$

		\item Show by contradiction that $r_{ \pm}$must be distinct.

		      Hint: if $r_{ \pm}=r$ are repeated, then $v_j=A r^j+B j r^j$ for some constants $A, B$.
		\item Since $r_{ \pm}$are distinct, the general solution for (1) is $v_j=A r_{+}^j+B r_{-}^j$ for constants $A, B$. Use this to conclude from (2) and (3) that,
		      $$
			      \left(\frac{b r_{+}^2}{c}\right)^{n+1}=1
		      $$

		\item From part (c), (3) and (4), show that $r_{ \pm}$must be complex valued and conclude that (4) has the solutions for $k=1, \ldots, n$,

		      $$
			      r_{ \pm, k}=\sqrt{\frac{c}{b}} \exp \left(\frac{ \pm i k \pi}{n+1}\right), \quad \text { where } i=\sqrt{-1}
		      $$

		\item Using part (e), conclude that the eigenvalues of $A$ is given by

		      $$
			      \lambda_k=a+2 \operatorname{sgn}\left(b  \right) \sqrt{bc} \cos \left(\frac{\pi k}{n+1}\right), \quad k=1, \ldots, n
		      $$
		\item
		      Find the eigenvalues of the $n \times n$ finite difference matrix $A_h=\frac{1}{h^2}\left(\begin{array}{ccccc}2 & -1 & && \\ -1 & 2 & \ddots&&  \\ & \ddots & \ddots \\ &&-1&2&-1 \\   &&\\ &&&-1&2 \end{array}\right)$, where $h=\frac{1}{n+1}$.

		      Conclude that $A_h$ is symmetric positive definite and find its condition number $\kappa\left(A_h\right)$ with respect to $\|\cdot\|_2$. Show that $\kappa\left(A_h\right)=\mathcal{O}\left(h^{-2}\right)$ as number of grid points $n$ increases. What does this mean for solving $A_h \boldsymbol{x}=\boldsymbol{b}$ when $n$ is large?

	\end{enumerate}
}
\begin{solution}
	\begin{enumerate}[label=(\alph*)]
		\item Let ($\lambda, \vec{v}$) be an eigenpair of $A$
		      \begin{align*}
			      A\vec{v}                                 & =   \lambda \vec{v} \\
			      \left( A - \lambda I \right)\vec{v}      & = \vec{0} \\
			      \left(\begin{array}{ccccc}
					            \left( a-\lambda \right) v_1 + bv_2   \\
					            cv_1 + (a-\lambda)v_2 + bv_3          \\
					            \vdots                                \\
					            cv_{n-2} + (a-\lambda) v_{n-1}+ b_{n} \\
					            c_{n-1} + (a-\lambda) v_{n}
				            \end{array}   \right) & = \vec{0} . \\
		      \end{align*}
		      We can write the above relation as the following,
		      \begin{equation}
			      cv_{j-1} + \left( a-\lambda \right) v_{j} + bv_{j+1} = 0  \label{req_rel}
			      . \end{equation}
		      Where  $0\le j \le n+1$ and $v_{0} = 0 = v_{n+1}$
		      \qed
		\item Using the hint we guess the following form of the solution $v_{j} = r^j$. Substituting in \ref{req_rel},
		      \begin{align*}
			      cr^{j-1} + \left( a-\lambda \right) r^{j} + br^{j+1} & =   0 \\
			      c + \left( a-\lambda \right) r + br^{2}              & =   0 \\
			      .\end{align*}
		      Using the quadratic formula, we get
		      \[
			      r_{\pm} = \frac{\lambda -a \pm \sqrt{(a-\lambda)^2 - 4bc} }{ 2b}.
		      \]
		      As $r_{\pm}$ are the roots to a quadratic, hence
		      \begin{equation}
			      r_{+}r_{-} = \dfrac{c}{b} \label{prod_rs}
		      \end{equation}
		\item If \ref{req_rel} has a repeated root, say $r_{\pm} = r $ , then solution to the recursion would look like,
		      \[
			      v_{j} = Ar^j + Bjr^j
			      .\]
		      Checking the boundary conditions,
		      $v_{0} = 0= v_{n+1}$
		      \begin{equation}
			      v_0 = A r^0 + B(0)r^{0} = A = 0 \label{v0}
			      .\end{equation}
		      \begin{equation}
			      v_{n+1} = (0) r^{n+1} + B(n+1)r^{n+1} = B(n+1)r^{n+1} = 0      \implies B =0 \label{vn+1}
			      .\end{equation}
		      Combining \ref{v0} \& \ref{vn+1} gives,
		      \[
			      v_j  =0
			      .\]
		      Which is the trivial eigenvector. Hence, we cannot have a repeated root if we want a non-zero eigenvector.
		\item From (c) we have that roots are distinct. Therefore, we look for solutions of the form $v_{j} = Ar_{+}^j + Br_{-}^j$ for some constants A and B defined by the "boundary conditions" of the recursion.
		      We have,
		      \begin{align}
			      v_{0}   & = A+B = 0 \implies A = -B    \nonumber \\
			      v_{n+1} & =  Ar_{+}^{n+1}  + Br_{-}^{n+1} =0 \implies r_{+}^{n+1} = r_{-}^{n+1} \label{rs_rel}
		      \end{align}
		      From, \ref{prod_rs} and \ref{rs_rel}, it follows that
		      \begin{align}
			      \left( r_{+}^{2} \right)^{(n+1)}          & =  \left( \frac{c}{b} \right)^{n+1} \nonumber \\
			      \left( \frac{br_{+}^2}{c} \right)^{(n+1)} & = 1 \label{rel_bcr}
		      \end{align}
		\item  We can observe in \ref{rel_bcr} that $\frac{br_{+}^{2}}{c}$ are the roots of unity, therefore,
		      \[
			      \frac{br_{+}^{2}}{c} = \exp\left( \frac{ik\pi}{n+1} \right) \implies r_{+} =  \sqrt{\frac{c}{b}} \exp\left( \frac{ik\pi}{n+1} \right) \quad k =0,\dots, n+1
			      .\]
		      Similarly,
		      \[
			      \frac{br_{-}^{2}}{c} = \exp\left( \frac{im\pi}{n+1} \right) \implies r_{-} =  \sqrt{\frac{c}{b}} \exp\left( \frac{im\pi}{n+1} \right) \quad m =0,\dots, n+1
			      .\]
		      Using \ref{prod_rs},
		      \[
			      r_{+}r_{-} =\frac{c}{b} \exp\left( \frac{i\left( k+m \right) \pi}{n+1} \right)=\frac{c}{b} \implies k=-m
			      .\]
		      Therefore we have,
		      \[
			      r_{\pm,k} = \sqrt{\frac{c}{b}} \exp\left( \frac{\pm ik\pi}{n+1} \right) \quad k=1,\dots, n
			      .\]
	\end{enumerate}
\end{solution}


\qs{ Classical iterative methods for strictly diagonally dominant matrices}{
	\begin{enumerate}[label=(\alph*)]
		\item Show that the diagonal part of any strictly diagonally dominant (S.D.D.) matrix is invertible.
		\item Recall the Gershgorin's theorem below, which can give useful information about the eigenvalues of a matrix. The eigenvalues of a complex valued matrix $A$ lies in the union of $n$ discs $\bigcup_{i=1}^n D_i$ on the complex plane, where
		      $$
			      D_i=\left\{z \in \mathbb{C}:\left|z-a_{i i}\right| \leq \sum_{j \neq i}\left|a_{i j}\right|\right\}
		      $$
		      Using Gershgorin's theorem, conclude S.D.D. matrices are invertible. Hint: Show that $0 \notin D_i$ for all $i=1, \ldots, n$.

		      The next two parts are about showing convergence of Jacobi and Gauss-Seidel iterations for S.D.D. matrices.
		\item Recall the matrix $-M^{-1} N$ associated with the Jacobi iteration takes the form $-D^{-1}(L+U)$, where $A=L+D+U$.
		      \begin{enumerate}[label=(\roman*)]
			      \item Let $A$ be S.D.D. and $\lambda$ be any eigenvalue of $-D^{-1}(L+U)$. Show that $\operatorname{det}(L+U+\lambda D)=0$ using part (a).
			      \item Now suppose $|\lambda| \geq 1$. Deduce from $A$ being S.D.D. that $L+U+\lambda D$ must also be S.D.D.
			      \item Deduce a contradiction by applying the result from part (b) to $L+U+\lambda D$, and conclude that $|\lambda|<1$.
			      \item Combine parts (i)-(iii) to conclude that Jacobi iteration converges for S.D.D. matrices.
		      \end{enumerate}
		\item Follow a similar argument as part (c) to show that Gauss-Seidel iterations converges for S.D.D. matrices.

	\end{enumerate}
}
\begin{solution}

	\begin{enumerate}[label=(\alph*)]
		\item Let $A $ be a S.D.D matrix and, 		      \[
			      \implies a_{ii} > \sum_{\substack{j=1\\j\neq i}}^{n} a_{ij} \ge  0 \implies a_{ii} >0 \quad \forall 1\le i \le n \\
			      .\]
		      Let $D$ be the matrix containing the diagonial entries of $A$, hence
		      \[
			      D =    \begin{bmatrix} a_{11} &        &        \\
                       & \ddots &        \\
                       &        & a_{nn}
			      \end{bmatrix}
			      .\]
		      As, all $a_{ii}>0$, therefore we $D^{-1}$ exists.
		\item Let $\lambda_i$ be the eigenvalues associated with disc $D_i$.\\
		      Suppose $0\in D_{i}$ for some $1\le i \le n $, therefore, we have,
		      \[
			      a_{ii} \leq \sum_{\substack{ j=0 \\ j\neq i }}^{n} a_{ij}
			      .\]
		      Which is false as $A$ is a S.D.D matrix, hence , $0 \not\in D_{i}$.\\
		      Therefore we have, $|\lambda_i| >0 \quad \forall i, 1\le i\le n \implies A^{-1}$ exists.
		\item \begin{enumerate}[label=(\roman*)]
			      \item Given that $\lambda$ is an eigenvalue of $-D^{-1}\left( L+U \right)$. Therefore we have $\vec v $ such that $\vec v \neq 0$,
			            \begin{align*}
				            -D^{-1}\left( L+U \right)\vec{v}  & =   \lambda \vec{{v}} \\
				            \left(L+U \right) \vec v          & =   -\lambda D\vec{{v}} \\
				            \left(L+U +\lambda D\right)\vec v & =  \vec{0}
				            .\end{align*}
			            As there is a non-zero null vector associated with $L+U+\lambda D $, therefore $\det(L+U+\lambda D) = 0$.
			      \item Given that $A$ is S.D.D.  Suppose $|\lambda| \ge 1$. Consider,
			            \begin{align*}
				            |(L+U+\lambda D )_{ii}|  = |\lambda a_{ii}| & =  |\lambda| | a_{ii}| \\
				                                                        & >  |\lambda| | \sum_{\substack{j=1 \\j\neq i}}^{n} a_{ij} | \\
				                                                        & \ge | \sum_{\substack{j=1          \\j\neq i}}^{n} a_{ij} | \\
				                                                        & = | \sum_{\substack{j=1            \\j\neq i}}^{n} (L+U+\lambda D )_{ij} | \\
			            \end{align*}
			            Hence, $\left( L+U+\lambda D  \right) $ is S.D.D. .
			      \item If $|\lambda| \ge  1 $ and $A$ is S.D.D, gives that $(L+U+\lambda D)$  is S.D.D .

			            Therefore, $(L+U+\lambda D )$ is invertible. Which is a contradiction as $det(L+U+\lambda D)= 0$. Therefore, $|\lambda| < 1$.
			      \item Let $M = D$ and $N=L+U$. From parts (i)-(iii) we get,
			            \[
				            \lambda_{i} \leq \lambda_{max} < 1 \implies \rho(-M^{-1}N) < 1
				            .\]
			            By theorem of convergence of iterative solvers we get, iterations based on $-M^{-1}N$ converges to $0$.

		      \end{enumerate}
		\item

		      \begin{enumerate} [label=(\roman*)]
			      \item Let $\lambda$ be an eigenvalue of $-(L+D)^{-1}\left( U \right)$. Therefore we have $\vec v $ such that $\vec v \neq 0$,
			            \begin{align*}
				            -(L+D)^{-1}\left( L+U \right)\vec{v} & =   \lambda \vec{{v}} \\
				            \left(U \right) \vec v               & =   -\lambda (L+D)\vec{{v}} \\
				            \left(U +\lambda (L+D)\right)\vec v  & =  \vec{0}
				            .\end{align*}
			            As there is a non-zero null vector associated with $U+\lambda \left( L+D \right)  $, therefore $\det(U+\lambda\left( L+D \right) ) = 0$.
			      \item Given that $A$ is S.D.D.  Suppose $|\lambda| \ge 1$. Consider,
			            \begin{align*}
				            |(U+\lambda(L+D) )_{ii}|  = |\lambda a_{ii}| & =  |\lambda| | a_{ii}| \\
				                                                         & >  |\lambda| | \sum_{\substack{j=1 \\j\neq i}}^{n} a_{ij} | \\
				                                                         & \ge |\lambda \sum_{j=1}^{i-1} a_{ij}  +  \sum_{j=i+1}^{n} a_{ij}  | \\
				                                                         & = | \sum_{\substack{j=1 \\j\neq i}}^{n} (U+\lambda (L+D) )_{ij} | \\
			            \end{align*}
			            Hence, $\left( U+\lambda(L+D)  \right) $ is S.D.D. .
			      \item If $|\lambda| \ge  1 $ and $A$ is S.D.D, gives that $(U+\lambda (L+D))$  is S.D.D .

			            Therefore, $(L+U+\lambda (L+D) )$ is invertible. Which is a contradiction as $det(U+\lambda (L+D))= 0$. Therefore, $|\lambda| < 1$.

			      \item Let $M =L+ D$ and $N=U$. From parts (i)-(iii) we get,
			            \[
				            \lambda_{i} \leq \lambda_{max} < 1 \implies \rho(-M^{-1}N) < 1
				            .\]
			            By theorem of convergence of iterative solvers we get, iterations based on $-M^{-1}N$ converges to $0$.

		      \end{enumerate}


	\end{enumerate}
\end{solution}

\qs{Classical iterative methods for symmetric positive definite matrices
}{

	This question is about coding and comparing classical iterative methods for the S.P.D. matrix $A_h$ from Q1(g).
	\begin{enumerate}[label=(\alph*)]
		\item Write a pseudocode for the classical iterative methods: Richardson, optimal Richardson, Jacobi, Gauss-Seidel, S.O.R., and optimal S.O.R.
		\item Implement a program to solve $A_h \boldsymbol{x}=\boldsymbol{b}$ with $\boldsymbol{b}=(1, \ldots, 1)^T \in \mathbb{R}^{20}$ and $\boldsymbol{x}_0=(1,0, \ldots, 0)^T \in \mathbb{R}^{20}$ using Richardson (with $\omega=\lambda_{\max }^{-1}$ ), optimal Richardson, Jacobi, Gauss-Seidel, S.O.R. (with $\theta=1.2$ ) and optimal S.O.R. Generate a plot comparing the log of their residual in $\ell_2$ norm versus iterations up to 5000 . Rank the performance of each method by comparing the iterations needed to reach the residual tolerance of $10^{-14}$. Use sparse representation when appropriate.\\ \textit{ Hint: Use Q1(g) to find parameters for Richardson and vary $\theta$ to find an approximate optimal parameter for S.O.R. }
		\item  Comment on the decreases in performance when $n=1000$. Explain briefly how this relates to $\kappa\left(A_h\right)=\mathcal{O}\left(h^{-2}\right)$.
	\end{enumerate}
}
\begin{solution}

	\begin{algorithm}[H]
		\Fn{\FuncSty{RichardsonIteration(}{A,b,$\bm x_0$,$\omega$,tol,maxIter }\FuncSty{)}}{
			\KwIn{\\A: The matrix to find the solution to \\  $\bm b$: The resultant vector in $A\bm x = \bm b$  \\ $\bm x_0$: The initial guess \\ $\omega$: Richardson parameter (fixed)  \\ maxIter: The maximum of iterations}
			\KwOut{x: The solution to $A\bm x = \bm b$}
			\SetAlgoLined
			\SetNoFillComment
			\vspace{3mm}
			$M \leftarrow  \omega^{-1}I$ \\
			$N \leftarrow A- M$\\
			$x  \leftarrow x_0$\\
			$\bm r  \leftarrow \bm b-A\bm x  $\\
			\While{$\|r\|_2 < tol$ \textbf{and} $i <$ maxIter } {
				$\bm x  \leftarrow \bm x + \omega \bm r  $\\
				$\bm r  \leftarrow \bm b - A\bm x$
			}
			\Return $\bm x$;
			\caption{Richardson Iteration}
		}
	\end{algorithm}

	\begin{algorithm}[H]
		\Fn(){\FuncSty{OptimalRichardsonIteration(}{A,$\bm b$,$\bm x_0$,tol,maxIter }\FuncSty{)}}{
			\KwIn{\\A: The matrix to find the solution to \\  $\bm b$: The resultant vector in $A\bm x = \bm b$  \\ $\bm x_0$: The initial guess \\  maxIter: The maximum of iterations}
			\KwOut{x: The solution to $A\bm x = \bm b$}
			\SetAlgoLined
			\SetNoFillComment
			\vspace{3mm}
			$\omega \leftarrow \dfrac{2}{\lambda_{max}\left(A\right) + \lambda_{min}\left(A\right) }$ \\
			\vspace{1mm}
			$M \leftarrow \omega^{-1}I $ \\
			$N \leftarrow A- M$\\
			$x  \leftarrow x_0$\\
			$\bm r = \bm b-A\bm x  $\\
			\While{$\|r\|_2 <$ tol \textbf{and} $i <$ maxIter} {
				$\bm x  \leftarrow \bm x + \omega \bm r  $\\
				$\bm r  \leftarrow \bm b - A\bm x$
			}
			\Return $\bm x$;
			\caption{Optimal Richardson Iteration}
		}
	\end{algorithm}

	\begin{algorithm}[H]
		\Fn(){\FuncSty{JacobiIteration(}{$A$,$\bm b$,$\bm x_0$,tol,maxIter }\FuncSty{)}}{
			\KwIn{\\$A$:The matrix to find the solution to \\  $\bm b$: The resultant vector in $A\bm x = \bm b$  \\ $\bm x_0$: The initial guess \\ maxIter: The maximum of iterations}
			\KwOut{x: The solution to $A\bm x = \bm b$}
			\SetAlgoLined
			\SetNoFillComment
			\vspace{3mm}
			$M \leftarrow $diag($A$) \\
			$N\leftarrow  A- M$\\
			$\bm x \leftarrow  \bm x_0$\\
			$\bm r \leftarrow \bm b-A\bm x  $\\
			\While{$\|r\|_2 <$ tol \textbf{and} $i <$ maxIter} {
				$\bm x  \leftarrow M^{-1}(\bm x + \bm b - N\bm x)  $\\
				$\bm r  \leftarrow \bm b - A\bm x$ \\
				$i = i+1$\\
			}
			\Return $\bm x$
			\caption{Jacobi Iteration}
		}
	\end{algorithm}

	\begin{algorithm}[H]
		\Fn(){\FuncSty{GaussSiedelIteration(}{A,$\bm b$,$\bm x_0$,tol,maxIter }\FuncSty{)}}{
			\KwIn{\\A: The matrix to find the solution to \\  $\bm b$: The resultant vector in $A\bm x = \bm b$  \\ $\bm x_0$: The initial guess \\  maxIter: The maximum of iterations}
			\KwOut{x: The solution to $A\bm x = \bm b$}
			\SetAlgoLined
			\SetNoFillComment
			\vspace{3mm}
			$M \leftarrow $diag($A$)+lower($A$) \\
			$N\leftarrow  A- M$\\
			$\bm x \leftarrow  \bm x_0$\\
			$\bm r \leftarrow \bm b-A\bm x  $\\
			$i \leftarrow 0$ \\
			\While{$\|r\|_2 <$ tol \textbf{and} $i <$ maxIter}  {
				$\bm x  \leftarrow M^{-1}(\bm x + \bm b - N\bm x)  $\\
				$\bm r  \leftarrow \bm b - A\bm x$ \\
				$i = i+1$
			}
			\Return $\bm x$;
			\caption{Gauss-Sidel Iteration}
		}
	\end{algorithm}
\end{solution}
\qs{Steepest Descent and Conjugate Gradient}
{
	\begin{enumerate}[label=(\alph*)]
		\item Let $A$ be a S.P.D. matrix. Show that $(\boldsymbol{x}, \boldsymbol{y})_A:=\boldsymbol{x}^T A \boldsymbol{y}$ for $\boldsymbol{x}, \boldsymbol{y} \in \mathbb{R}^n$ forms an inner product on $\mathbb{R}^n$.
		\item Using part (a), conclude that $\|\boldsymbol{x}\|=(\boldsymbol{x}, \boldsymbol{x})_A^{1 / 2}$ for $\boldsymbol{x} \in \mathbb{R}^n$ is a norm on $\mathbb{R}^n$.

		      Hint: You can assume the Cauchy-Schwarz inequality $\left|(\boldsymbol{x}, \boldsymbol{y})_A\right| \leq\|\boldsymbol{x}\|_A\|\boldsymbol{y}\|_A$ holds.
		\item For the method of Steepest Descent, show that $\nabla f\left(\boldsymbol{x}_k\right)$ and $\nabla f\left(\boldsymbol{x}_{k+1}\right)$ are orthogonal (i.e. zig-zaging behavior), where $f(\boldsymbol{y})=\frac{1}{2} \boldsymbol{y}^T A \boldsymbol{y}-\boldsymbol{y}^T \boldsymbol{b}$. Hint: Recall how the step size for Steepest Descent is determined.
		\item Repeat the experiment from $\mathbf{Q 3}(\mathbf{b})$ with $\boldsymbol{b}=(1, \ldots, 1)^T \in \mathbb{R}^{1000}$ and $\boldsymbol{x}_0=(1,0, \ldots, 0)^T \in \mathbb{R}^{1000}$ using the method of Steepest Descent and Conjugate Gradient. Generate a plot comparing the $\log$ of their residual in $\ell_2$ norm versus iterations up to 5000 . Rank their performance by comparing the iterations needed to reach the residual tolerance of $10^{-14}$, as well as versus the classical iterative methods. Verify your CG method terminates after the desired number of iterations. Use sparse representation when appropriate.
	\end{enumerate}
}
\begin{solution}
	\begin{enumerate}[label=(\alph*)]
		\item $\left( .,. \right) $ is an inner-product if :
		      \begin{enumerate}[label=(\roman*)]
			      \item  Conjugate Symmetery: \\
			            \[
				            \left( x,y \right)_{A}  = {\left(y,x\right)_A}
				            .\]
			      \item Linearity \\
			            \[
				            \left( a\vec{x}+b\vec{y}, \vec z \right)_A  = a \left( \vec x,\vec z  \right)_A +b\left(\vec  y,\vec z \right)_A
				            .\]
			      \item Positive-Definiteness:\\
			            \[
				            \left( \vec x, \vec x \right)_A  > 0
				            .\]
		      \end{enumerate}
		      \begin{enumerate}[label = (\roman*)]
			      \item \[
				            \left( x,y \right)_A = x^{T}Ay = y^{T}Ax = \left( y,x \right)_A
				            .\]
			      \item
			            \[
				            \left( a\vec{x}+b\vec{y}, \vec z \right)_A = (a\vec{x}+b\vec{y})^{T} A \vec z=  a\vec{x}^{T} A \vec z+b\vec{y}^{T} A \vec z  = a \left( \vec x,\vec z  \right)_A +b\left(\vec  y,\vec z \right)_A
				            .\]
			      \item
			            \[
				            \left( x,x \right)_A = \vec x^{T} A  \vec x >0 \quad \text{As $A$ as is SPD}
				            .\]
			            Therefore, $\left( .,. \right)_A $ is an inner-product.
		      \end{enumerate}
		\item $\|.\|_A$ is an norm if :
		      \begin{enumerate}[label=(\roman*)]
			      \item  Positive Definitness : \\
			            \[
				            \|x\|_A > 0 \quad \forall \vec x \neq 0 \quad \land \quad \|\vec x\|_A = 0 \iff \vec x = \vec 0
				            .\]
			      \item Scalar Multiplication\\
			            \[
				            \|\lambda \vec x \|_A = \lambda \|\vec x\|_A
				            .\]
			      \item Sub-additivity (Triangle Inequality):\\
			            \[
				            \|\vec x+ \vec y\|_A = \| \vec x\|_A  + \| \vec y \|_A
				            .\]

		      \end{enumerate}

		      \begin{enumerate}[label=(\roman*)]
			      \item  	Let $x\in \RR^{n} $ and $\vec x \neq \vec 0$    \[
				            \|x\|_A = \sqrt{\vec x^{T}A\vec x}  >0  \quad \text{,as $A$ is SPD}
				            .\]
			            Let $\|\vec x \|_A =0$
			            \[
				            \|\vec x \|_A =0 = \sqrt{\vec x^{T} A \vec x}  \iff \vec x = 0 \quad \text{, as $A$ is SPD}
				            .\]

			      \item Scalar Multiplication\\
			            \[
				            \|\lambda \vec x \|_A = \sqrt{\lambda \vec x^{T} A \lambda \vec x} = \sqrt{\lambda^{2} \vec x^{T} A \vec x}    = \lambda \|\vec x\|_A
				            .\]
			      \item Sub-additivity (Triangle Inequality):\\
			            \begin{align*}
				            \|\vec x+ \vec y\|_A
				             & = \sqrt{\left( \vec x+ \vec y \right)^{T} A \left( \vec x+ \vec y \right)  } \\
				             & =  \sqrt{\vec x^{T}A\vec x + \vec x^{T}A\vec y + \vec y^{T}A\vec x +\vec y^{T}A\vec y  } \\
				             & = \sqrt{\vec x^{T}A\vec x + 2\vec x^{T}A\vec y +\vec y^{T}A\vec y  } \\
				             & =   \sqrt{\|\vec x\|_A^2+ 2 \left( \vec x,\vec y \right)  +\|\vec y\|_A^2 } \\
				             & \le  \sqrt{\|\vec x\|_A^{2}+ 2 \|\vec x\|_A\|\vec y\|_A   +\|\vec y\|_A^2 } \\
				             & =  \| \vec x\|_A  + \| \vec y \|_A
				            .\end{align*}
			            Therefore, $\|x\|_A$ is valid norm.
		      \end{enumerate}
	\end{enumerate}
\end{solution}
\end{document}
